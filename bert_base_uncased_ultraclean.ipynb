{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MAX_LEN = 64        # lyrics truncated/padded to 256 tokens\n",
        "NUM_CLASSES = 2      # low / mid / high valence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/muse_d.csv\")\n",
        "\n",
        "df[\"lyrics\"] = df[\"lyrics\"].astype(str)\n",
        "df[\"valence_tags\"] = pd.to_numeric(df[\"valence_tags\"], errors=\"coerce\")\n",
        "df=df[[\"lyrics\",\"valence_tags\"]]\n",
        "\n",
        "def valence_to_label(v):\n",
        "    if v <= 5:   return int(0)          # low‐valence\n",
        "    elif v > 5:  return int(1)          # high\n",
        "\n",
        "df[\"label\"] = df[\"valence_tags\"].apply(valence_to_label)\n",
        "df.dropna(inplace=True)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def clean_lyrics(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Collapse whitespace\n",
        "    text = re.sub(r\"’\", \"'\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9',.!?()\\s]\", \"\", text)  # Keep basic punctuation\n",
        "    return text.strip().lower()\n",
        "\n",
        "def preprocess_train_val(df, val_size=0.2, random_state=42):\n",
        "    df = df.dropna(subset=[\"lyrics\", \"label\"]).copy()\n",
        "    df[\"lyrics\"] = df[\"lyrics\"].apply(clean_lyrics)\n",
        "\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=val_size,\n",
        "        stratify=df[\"label\"],\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "train_df, val_df= preprocess_train_val(df, val_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts  = df[\"lyrics\"].tolist()\n",
        "        self.labels = df[\"label\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\":      encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\":         torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "train_ds = LyricsDataset(train_df.reset_index(drop=True))\n",
        "val_ds   = LyricsDataset(val_df.reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Sequential(\n",
        "            nn.Linear(dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B, L, D)\n",
        "        weights = self.attn(x).squeeze(-1)             # (B, L)\n",
        "        weights = torch.softmax(weights, dim=1)        # (B, L)\n",
        "        pooled = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (B, D)\n",
        "        return pooled\n",
        "\n",
        "class BertLyricsClassifier(nn.Module):\n",
        "    def __init__(self, num_labels=NUM_CLASSES, model_name=MODEL_NAME, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.pool = AttentionPooling(768)\n",
        "        self.norm = nn.LayerNorm(768)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B, L, 768)\n",
        "        pooled = self.pool(out)       # (B, 768)\n",
        "        pooled = self.norm(pooled)\n",
        "        return self.head(pooled)      # logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class BertLyricsClassifier(nn.Module):\n",
        "    def __init__(self, num_labels=NUM_CLASSES, model_name=MODEL_NAME, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.norm = nn.LayerNorm(768)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B, L, 768)\n",
        "        out = out.permute(0, 2, 1)             # (B, 768, L)\n",
        "        pooled = self.pool(out).squeeze(-1)    # (B, 768)\n",
        "        pooled = self.norm(pooled)\n",
        "        return self.head(pooled)               # logits'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "'''class BertLyricsClassifier(nn.Module):\n",
        "    def __init__(self, num_labels=NUM_CLASSES, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        if freeze_bert:\n",
        "            for p in self.bert.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.enc = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=768, nhead=12, dim_feedforward=2048, dropout=0.1\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.norm = nn.LayerNorm(768)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        out = self.enc(out)                     # (B, L, 768)\n",
        "        out = out.permute(0, 2, 1)             # to (B, 768, L)\n",
        "        pooled = self.pool(out).squeeze(-1)    # (B, 768)\n",
        "        pooled = self.norm(pooled)\n",
        "        return self.head(pooled)               # logits'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = BertLyricsClassifier(freeze_bert=False).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "# Discriminative LRs: tiny for BERT, bigger for new layers\n",
        "base_params   = [p for n,p in model.named_parameters() if n.startswith(\"bert\")]\n",
        "head_params   = [p for n,p in model.named_parameters() if not n.startswith(\"bert\")]\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": base_params, \"lr\": 1e-5},\n",
        "    {\"params\": head_params, \"lr\": 3e-5}\n",
        "], weight_decay=1e-2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=1.0, end_factor=0.1, total_iters=EPOCHS*len(train_loader)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    val_loss, preds, truths = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            ids  = batch[\"input_ids\"].to(DEVICE)\n",
        "            mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "            y    = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "            logits = model(ids, mask)\n",
        "            loss   = criterion(logits, y)\n",
        "            val_loss += loss.item() * ids.size(0)\n",
        "\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().tolist())\n",
        "            truths.extend(y.cpu().tolist())\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    acc  = accuracy_score(truths, preds)\n",
        "    f1   = f1_score(truths, preds, average=\"macro\")\n",
        "    return val_loss, acc, f1\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "    for batch in pbar:\n",
        "        ids  = batch[\"input_ids\"].to(DEVICE)\n",
        "        mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        y    = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "        logits = model(ids, mask)\n",
        "        loss   = criterion(logits, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item() * ids.size(0)\n",
        "        pbar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "    val_loss, acc, f1 = evaluate()\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train_loss={epoch_loss:.4f} | val_loss={val_loss:.4f} \"\n",
        "          f\"| val_acc={acc:.3f} | val_f1={f1:.3f}\")\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}